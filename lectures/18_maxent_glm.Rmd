---
title:
css: style.css
output:
  revealjs::revealjs_presentation:
    reveal_options:
      slideNumber: true
      previewLinks: true
    theme: white
    center: false
    transition: fade
    self_contained: false
    lib_dir: libs
---
##
<center>
<h2>Maximum Entropy and Generalized Linear Models</h2>
</center>
\
![](./images/18/glm_cat.jpg){width="50%"}


```{r prep, echo=FALSE, cache=FALSE, message=FALSE, warning=FALSE}
library(knitr)

opts_chunk$set(fig.height=5, fig.width=7, comment=NA, 
               warning=FALSE, message=FALSE, 
               dev="jpeg", echo=FALSE)

library(rethinking)
library(dplyr)
library(tidyr)
library(ggplot2)
#center plot titles
theme_set(theme_bw())
theme_update(plot.title = element_text(hjust = 0.5))

```

## What is Maximum Entropy?
![](./images/18/tangled-ball-mess-of-computer-cords-dxpt68.jpg)

## Why are we thinking about MaxEnt?
- MaxEnt distributions have the widest spread - conservative  
\
- Nature tends to favor maximum entropy distributions  
     - It's just natural probability
\
- The foundation of Generalized Linear ModelDistributions  
\
- Leads to usefu distributions once we impose *constraints*

##
![](./images/18/maxent_1.jpg)
<div style="font-size:11pt">McElreath 2016</div>

##
![](./images/18/maxent_2.jpg)
<div style="font-size:11pt">McElreath 2016</div>

##
![](./images/18/maxent_3.jpg)
<div style="font-size:11pt">McElreath 2016</div>

##
![](./images/18/maxent_4.jpg)
<div style="font-size:11pt">McElreath 2016</div>

##
![](./images/18/maxent_5.jpg)
<div style="font-size:11pt">McElreath 2016</div>

##
![](./images/18/maxent_6.jpg)
<div style="font-size:11pt">McElreath 2016</div>

##
![](./images/18/maxent_7.jpg)
<div style="font-size:11pt">McElreath 2016</div>

##
![](./images/18/maxent_8.jpg)
<div style="font-size:11pt">McElreath 2016</div>

## Information Entropy
$$H(p) = - \sum p_i log \, p_i$$

- Measure of uncertainty  
\
- If more events possible, it increases  
\
- Nature finds the distribution with the largest entropy, given constraints of distribution

## Maximum Entropy and Coin Flips
* Let's say you are flipping a fair (p=0.5) coin twice    
\
* What is the maximum entropy distribution of # Heads?  
\
* Possible Outcomes: TT, HT, TH, HH  
     - which leads to 0, 1, 2 heads  
\
* Constraint is, with p=0.5, the average outcome is 1 heads
     
## The Binomial Possibilities
```{r entropy}
entropy <- function(z) {
  ret <- -1*sum(sapply(z, function(x) x*log(x)))
  if(!is.finite(ret)) ret <- 0
  ret
}
expandBin <- function(p, trials){
  ways <- sapply(0:trials, function(x) choose(trials,x))
  out <- sapply(1:length(p), function(i) rep(p[i]/ways[i], times=ways[i]))
  do.call(c, out)
}

binEnt <- function(trials = 2, prob=0.5){
  p <- dbinom(0:trials, trials, prob)
  entropy(expandBin(p, trials))
}

```
TT = p<sup>2</sup>  
HT = p(1-p)  
TH = (1-p)p  
HH = p<sup>2</sup>  
\
<center>But other distributions are possible</center>

## Let's compare to some other distributions meeting constraint using Entropy
$$H = - \sum{p_i log p_i}$$\

| Distribution| TT, HT, TH, HH | Entropy|
|------------|----------------|----------|
|Binomial | 1/4, 1/4, 1/4, 1/4  | `r round(entropy(rep(0.25,4)),3)` |
|Candiate 1 | 2/6, 1/6, 1/6, 2/6 |  `r round(entropy(c(2/6, 1/6, 1/6, 2/6)),3)` |
|Candiate 2 | 1/6, 2/6, 2/6, 1/6 |  `r round(entropy(c(1/6, 2/6, 2/6, 1/6)),3)` |
|  Candiate 3 | 1/8, 1/2, 1/8, 2/8 |  `r round(entropy(c(1/8, 1/2, 1/8, 2/8)),3)` |

\
<center>Binomial wins!</center>

## What about other p's and draws?
Assume 2 draws, p=0.7, make 1000 simulated distributions
```{r bin_try}

get_rand_coin_dist <- function(trials=2, prob=0.5, expand=TRUE){
  exp_heads <- sum(dbinom(0:trials, trials, prob)*0:trials)
  x <- runif(trials) #gets 0, 1, ... trials -1
  r <- sum(sapply(1:trials, function(j) x[j]*(j-1))) #intermediate thing
  x_i <- (sum(x)*exp_heads - r)/(trials - exp_heads) #the constraint
  p <- c(x,x_i)/sum(c(x,x_i)) #normalize to probabilities
  
  #test
  #exp_heads
  #sum(p*0:trials)

  #return
  if(expand) return(expandBin(p,trials))
  p
}

H_2_0.7 <- replicate(1e3, entropy(get_rand_coin_dist(trials = 2, prob=0.7)))
plot(density(H_2_0.7, bw=0.001), main=paste("Trials=100, prob=0.6\nBinomial Max Ent = ", binEnt(2, 0.7)), xlab="Entropy")
```


## What about other p's and draws?
Assume 10 draws, p=0.6, make 1000 simulated distributions
```{r maxent2, cache=TRUE}
H_10_0.6 <- replicate(1e3, entropy(get_rand_coin_dist(10, 0.9)))
plot(density(H_10_0.6, bw=0.01), main=paste("Trials=10, prob=0.6\nBinomial Max Ent = ", binEnt(10, 0.6)), xlab="Entropy")
```

## OK, what about the Gaussian?
<div id="left">
```{r norm, fig.height=6,fig.width=5}
library(pgnorm)
norm_dists <- crossing(x=seq(-4,4, length.out=300), shape=1:4) %>%
  rowwise() %>%
  mutate(y=dpgnorm(x, shape, 0,1)) %>%
  ungroup()

qplot(x, y, data=norm_dists, color=factor(shape), geom="line") +
  ggtitle("Generalized Normal Distribution\nStandard Normal Shape=2")
```
</div>

<div id="right">
- Constraints: mean, finite variance, unbounded  
\
- Lots of possible distributions for normal processes\
\
- Flattest distribution given constraints: MaxEnt
</div>

## Maximum Entropy Distributions
Constraints | Maxent distribution |
------------|---------------------|
Real value in interval | Uniform |
Real value,  finite variance | Gaussian |
Binary events,  fixed probability | Binomial |
Non-negative real, has mean | **Exponential** |

Uniform Gaussian Binomial Exponential

## How Distributions are Coupled
![](./images/18/all_dists.jpg)
```{r dists, echo=TRUE}
#for example
replicate(1e5, sum(rbinom(10, size=100, prob=0.01)))
```
\
<div class="fragment">**ENTER GENERALIZED LINEAR MODELS**</div>

## Our Models Until Now
Likelihood:  
$y_i \sim Normal(\mu_i, \sigma)$  
\
Data Generating Process  
$\mu_i = \alpha + \beta_1 x1_i + \beta_2 x2_i + ...$  
\
Priors:  
$\alpha \sim Normal(0, 100)$  
$\beta_j \sim Normal(0, 100)$    
$\sigma \sim cauchy(0,2)$  

## Making the Normal General
Likelihood:  
$y_i \sim Normal(\mu_i, \sigma)$  
\
Data Generating Process with identity link  
f($\mu_i) = \alpha + \beta_1 x1_i + \beta_2 x2_i + ...$  
\
Priors:  
...  

## A Generalized Linear Model
Likelihood:  
$y_i \sim D(\theta_i, ...)$  
\
Data Generating Process with identity link  
f($\theta_i) = \alpha + \beta_1 x1_i + \beta_2 x2_i + ...$  
\
Priors:  
...  

## The Exponential Family is MaxEnt!
![](./images/18/all_dists.jpg)

## How to determine which non-normal distribution is right for you
* Use previous image to determine  
\
* Counts: Poisson, binomial, multinomial, geometric  
\
* Distances and durations: exponential, gamma (survival or event history)
\
* Monsters: Ranks and ordered categories  
\
* Mixtures: Beta-binomial, gamma-Poisson, zero- inflated processes

## Binomial Logistic Regression
Likelihood:  
$y_i \sim B(size, p_i)$  
\
Data Generating Process with identity link  
logit($p_i) = \alpha + \beta_1 x1_i + \beta_2 x2_i + ...$  
\
Priors:  
...  

## Why Binomial Logistic Regression
- Allows us to predict **absolute** probability of something occuring  
\
- Allows us to determing **relative** change in risk due to predictors

## Why a Logit Link?
![](./images/18/logit_conversion.jpg)
<div style="font-size:11pt">McElreath 2016</div>

## Meaning of Logit Coefficients
$$logit(p_i) = log \frac{p_i}{1-p_i} = \alpha + \beta x_i$$

- $\frac{p_i}{1-p_i}$ is *odds* of something happening  
\
- $\beta$ is change in *log odds* per one unit change in $x_i$  
     - exp($\beta$) is change in odds  
     - Change in **relative risk**  
\
- $p_i$ is *absolute probability* of something happening
     - logistic($\alpha + \beta x_i$) = probability
     - To evaluate change in probability, choose two different $x_i$ values

## Binomial GLM in Action: Gender Discrimination in Graduate Admissions
![](./images/18/FINAL_gender_news_dani-01.jpg)

## Our data: Berkeley
```{r ucb, echo=TRUE}
data(UCBadmit)
head(UCBadmit)
```

## 
\
\
\
<h1>What model would you build?</h1>

## One Model
```{r ecb_mod1, echo=TRUE}
#female = 1, male = 2
UCBadmit$gender <- as.numeric(UCBadmit$applicant.gender)

mod_gender <- alist(
  #likelihood
  admit ~ dbinom(applications, p),
  
  #Data generating process
  logit(p) <- a[gender],
  
  #priors
  a[gender] ~ dnorm(0,10)
)

fit_gender <- map(mod_gender, UCBadmit)
```

## Results... men do better!
```{r admit_precis, echo=TRUE}
#female = 1, male = 2
precis(fit_gender, depth=2)
```

<div class="fragment">
What it means
```{r p, echo=TRUE}
#Women
logistic(-0.83)

#Men
logistic(-0.22)
```

## Relative comparison: Male Advantage
```{r male_adf, echo=TRUE}
samp <- extract.samples(fit_gender)

exp(mean(samp$a[,2] - samp$a[,1]))
```

```{r plot_diff}
plot(density(samp$a[,2] - samp$a[,1]), main="Male Advantage Distribution",
     xlab="Male - Female Coefficient")
```

## But was this a good model?
```{r postcheck, cache=TRUE, results="hide"}
postcheck( fit_gender , n=1e4 )

# draw lines connecting points from same dept
for ( i in 1:6 ) {
    x <- 1 + 2*(i-1)
    y1 <- UCBadmit$admit[x]/UCBadmit$applications[x]
    y2 <- UCBadmit$admit[x+1]/UCBadmit$applications[x+1]
    lines( c(x,x+1) , c(y1,y2) , col=rangi2 , lwd=2 )
    text( x+0.5 , (y1+y2)/2 + 0.05 , UCBadmit$dept[x] , cex=0.8 , col=rangi2 )
}
```
Could also have looked at pure residuals by department - but `postcheck` will do here

## Quantiles of Residuals via Simulation
- Remember that QQ Norm plots don't work?  
\
- But we can get quantiles of predictions relative to posterior prediction

```{r post_pred_q_1, echo=TRUE}
#sim gets predictions
post_pred <- sim(fit_gender, refresh=0)

#figure out how much of each column < observation
quants <- sapply(1:nrow(UCBadmit), function(i){
  #what fraction of predictions < the observation
  sum(post_pred[,i] < UCBadmit$admit[i])/nrow(post_pred)
})
```

## QQ Unif Looks..not great....
```{r post_pred_qq_plot, echo=TRUE}
#get the quantiles from the uniform distribution
gap::qqunif(quants, logscale=FALSE)
```

## So, department...

```{r ecb_mod2, echo=TRUE}
#Make a dept index
UCBadmit$dept_id <- as.numeric(UCBadmit$dept)

mod_gender_dept <- alist(
  #likelihood
  admit ~ dbinom(applications, p),
  
  #Data generating process
  logit(p) <- a[dept_id] + b[gender],
  
  #priors
  a[dept_id] ~ dnorm(0,10),
  b[gender] ~ dnorm(0,10)
)

fit_gender_dept <- map(mod_gender_dept, UCBadmit)
```

## Did it work?
```{r postcheck2, echo=TRUE, cache=TRUE, results="hide"}
postcheck(fit_gender_dept)
```

## QQ Unif says yes!
```{r post_pred_q, echo=FALSE}
#sim gets predictions
post_pred_2 <- sim(fit_gender_dept, refresh=0)

#figure out how much of each column < observation
quants_2 <- sapply(1:nrow(UCBadmit), function(i){
  #what fraction of predictions < the observation
  sum(post_pred_2[,i] < UCBadmit$admit[i])/nrow(post_pred_2)
})

plot(quants_unif, sort(quants_2))
abline(a=0, b=1)
```

## Outcomes
```{r gender}
precis(fit_gender_dept, depth=2)
```

\
\
<div class="fragment">What do coefficients mean?</div>  
<div class="fragment">Note that difference has reversed!</div>
\
<div class="fragment"><center>**Simpson's Paradox!**</center></div>

## Exercise/For Thursday
**The examples:** Poisson, Multinomial, and Geometric examples from book, Gamma example of change in experimental duration over time  
\
**The task:** Make RMarkdown slides that do the following:  
\

1. Introduce the distribution  
2. Introduce how it fits into the GLM framework  
3. Introduce a data set  
4. Show an example of fitting the data  
       - Show posterior evaluation  
       - Compare to STAN fit  
       - Show predictions  
       - How do we interpret results  
       
\
Email me .html, .Rmd and accompanying files after class!