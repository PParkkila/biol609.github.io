---
title:
css: style.css
output:
  revealjs::revealjs_presentation:
    reveal_options:
      slideNumber: true
      previewLinks: true
    theme: white
    center: false
    transition: fade
    self_contained: false
    lib_dir: libs
---
##
<center>
<h2>Bayesian Analysis with Multiple Predictors</h2>
</center>
\
![](./images/14/mult_reg_dog.jpg){width="50%"}


```{r prep, echo=FALSE, cache=FALSE, message=FALSE, warning=FALSE}
library(knitr)

opts_chunk$set(fig.height=5, fig.width=7, comment=NA, 
               warning=FALSE, message=FALSE, 
               dev="jpeg", echo=FALSE)

library(rethinking)
library(dplyr)
library(tidyr)
library(ggplot2)
data(Howell1)
Howell1_Adult <- Howell1 %>% filter(age >= 18)
```


## Why Linear Regression: A Simple Statistical Golem
```{r fig, fig.height=3, fig.width=5}
qplot(height, weight, data=Howell1_Adult) + stat_smooth(method="lm") +
  theme_bw(base_size=17)
```

- Describes association between predictor and response  
- Response is additive combination of predictor(s)  
- Constant variance  


## Why should we be wary of linear regression?
```{r fig, fig.height=3, fig.width=5}
```

- Approximate  
- Not mechanistic   
- Often deployed without thought  
- But, often very accurate

## {data-background="images/14/wafflehouse.jpg"}

## {data-background="images/14/2014-05-15-map1Nik.jpg"}
<!-- from http://www.huffingtonpost.com/nik-freeman/waffle-house-locations_b_5332044.html -->

## Waffle House: Does it Lead to Perdition?
```{r waffle_data}
data(WaffleDivorce)
qplot(WaffleHouses, Divorce, data=WaffleDivorce) +
  stat_smooth(method="lm") +
  theme_bw(base_size=17) +
  ylab("Divorce Rate") + xlab("Waffle Houses per Million People")
```

## So many possibilities
```{r waffle_pairs}
with(WaffleDivorce,
     pairs(cbind(Divorce, MedianAgeMarriage, Marriage, WaffleHouses, Slaves1860)))
```


## Today's Outline
1. Multiple Predictors in a Bayesian Framework  
    - How multiple predictors tease apart spurious and masked relationships  
\
2. Evaluating a Model with Multiple Predictors  
\
3. Problems With Too Many Predictors  
\
4. Categorical Variables

## Why use multiple predictors
<div id="right">
1. Controlling for confounds  
    - Disentangle spurious relationships
    - Reveal masked relationships  
\
2. Dealing with multiple causation  
\
3. Interactions (soon)

</div>

<div id="left">

```{r waffle_pairs2}
with(WaffleDivorce,
     pairs(cbind(Divorce, MedianAgeMarriage, Marriage)))
```

</div>

## Why *NOT* use multiple predictors
<div id="right">
1. Multicollinearity  
\
2. Overfitting  
\
3. Loss of precision in estimates  
\
4. Interpretability
</div>

<div id="left">

```{r waffle_pairs2}
```

</div>


## How to Build a Model with Multiple Predictors
Likelihood:  
$h_i \sim Normal(\mu_i, \sigma)$  
\
Data Generating Process  
$\mu_i = \alpha + \beta_1 x1_i + \beta_2 x2_i + ...$  
\
Prior:  
$\alpha \sim Normal(0, 100)$  
$\beta_j \sim Normal(0, 100)$    
$\sigma \sim U(0,50)$  


## Our Data
```{r waffle_pairs2}
```

## Let's start with standardization
```{r waffleStandard, echo=TRUE}
WaffleDivorce <- WaffleDivorce %>%

  #using scale to center and divide by SD
  mutate(Marriage.s = (Marriage-mean(Marriage))/sd(Marriage),
         MedianAgeMarriage.s = as.vector(scale(MedianAgeMarriage)))
```

## Can we Trust This?
```{r waffle_marraige}
qplot(Marriage.s, Divorce, data=WaffleDivorce) +
  stat_smooth(method="lm") +
  theme_bw(base_size=17) 
```

## Can we Trust This?
```{r waffle_median}
qplot(MedianAgeMarriage.s, Divorce, data=WaffleDivorce) +
  stat_smooth(method="lm") +
  theme_bw(base_size=17) 
```

## If these are correlated, which is the driver?
```{r marraige_median}
qplot(MedianAgeMarriage.s, Marriage.s, data=WaffleDivorce) +
  stat_smooth(method="lm") +
  theme_bw(base_size=17) 
```

## What does a Multiple Regression Coefficient Mean?
- What is the predictive value of one variable once all others have been accounted for?  
\
- We want a coefficient that explains the unique contribution of a predictor  
\
- What is the effect of x1 on y after we take out the effect of x2 on x1?  

## Our Model
Likelihood:  
$D_i \sim Normal(\mu_i, \sigma)$  
\
Data Generating Process  
$\mu_i = \alpha + \beta_R R_i + \beta_A A_i$  
\
Prior:  
$\alpha \sim Normal(10, 10)$  &nbsp; &nbsp; <span class="fragment">Guess from data</span>  
$\beta_R \sim Normal(0, 1)$   &nbsp; &nbsp; <span class="fragment">Because standardized</span>   
$\beta_A \sim Normal(0, 1)$   &nbsp; &nbsp; <span class="fragment">Because standardized</span>  
$\sigma \sim U(0,10)$  &nbsp; &nbsp; <span class="fragment">Guess from data</span>  

## Our Model
```{r wafffleDivorce_model, echo=TRUE}
mod <- alist(
  #likelihood
  Divorce ~ dnorm(mu, sigma),
  
  #data generating process
  mu <- a + bR*Marriage.s + bA * MedianAgeMarriage.s,
  
  # Priors
  a ~ dnorm(10,10),
  bR ~ dnorm(0,1),
  bA ~ dnorm(0,1),
  sigma ~ dunif(0,10)
)

fit <- map(mod, data=WaffleDivorce)
```

## Results: We Only Need Median Age
```{r waffleRes}
precis(fit)

plot(precis(fit))
```

## Today's Outline
1. Multiple Predictors in a Bayesian Framework  
    - How multiple predictors tease apart spurious and masked relationships  
\
2. **Evaluating a Model with Multiple Predictors**  
\
3. Problems With Too Many Predictors  
\
4. Categorical Variables


## How to Understand Posteriors
```{r wafflesPostPair, echo=TRUE}
pairs(fit)
```

## How to Understand Posteriors
1. Predictor-residual plots  
    - What if you remove the effect of other predictors?  
\
2. Counterfactual plots
    - What if something else had happened?  
\
3. Posterior Predictions  
    - How close are model predictions to the data

## Predictor-Residual Plots
- The `cr.plots` from the `car` package  
    - Component-residual  
\
- Take the residual of a predictor, assess it's predictive power  
\

## Steps to Make Predictor-Residual Plots
1. Compute predictor 1 ~ all other predictors  
\
2. Take residual of predictor 1  
\
3. Regress predictor 1 on response

## PR Model Part 1
```{r marraiges_resid, echo=TRUE}
m_mod <- alist(
  #model
  Marriage.s ~ dnorm(mu, sigma),
  mu <- a + b*MedianAgeMarriage.s,
  
  #priors
  a ~ dnorm(0,10),
  b ~ dnorm(0,10),
  sigma ~ dunif(0,10))

m_fit <- map(m_mod, data=WaffleDivorce)
```

## PR Model Part 2
```{r get_resid, echo=TRUE}
WaffleDivorce <- WaffleDivorce %>%
  mutate(Marriage_resid = Marriage.s - 
           (coef(m_fit)[1] + coef(m_fit)[2]*MedianAgeMarriage.s)
)
```

## The Predictor-Residual Plot
```{r waf_cr}
qplot(Marriage_resid, Divorce, data=WaffleDivorce) +
  stat_smooth(method="lm") +
  theme_bw(base_size=17) +
  xlab("Marraige Rate after accounting for Median Age") +
  geom_vline(xintercept=0, lty=2)

#Yeah, I know I should have refit a MAP model - but, this works
```

<div class="fragment">What have we learned after accounting for median age?</div>

## Counterfactual Plots
- Counterfactual: A conditional statement of "if this, then ..."  
\
- Powerful way of assessing models - "If we had seen Marraige Rate as x, then the effect of Median age on divorce rate would be..."  
\
- Shows model implied predictions, often at levels nor observed

## Counterfactual Plots: Code
```{r get_conterfact, echo=TRUE, cache=TRUE, style="font-size:0.3em"}
cf_data <- crossing(Marriage.s = -2:2, 
                    MedianAgeMarriage.s = seq(-2,2,length.out=300))
#get the data
cf_mu <- link(fit, data = cf_data, refresh=0)
cf_pred <- sim(fit, data=cf_data, refresh=0)

#Get the mean trend
cf_data$Divorce = apply(cf_mu, 2, median)

#get the intervals
cf_mu_pi <- apply(cf_mu, 2, HPDI)
cf_pred_pi <-apply(cf_pred, 2, HPDI)

#add back to the data
cf_data <- cf_data %>% 
  mutate(mu_lwr = cf_mu_pi[1,], mu_upr = cf_mu_pi[2,],
         pred_lwr = cf_pred_pi[1,], pred_upr = cf_pred_pi[2,])
```

## What do we learn about the effects of Median Marriage Age?
```{r cf_plot}
ggplot(data=cf_data %>% filter(Marriage.s==0), 
       mapping=aes(x=MedianAgeMarriage.s, y=Divorce,
                   ymin = pred_lwr, ymax = pred_upr)) +
  geom_ribbon(alpha=0.2) +
  geom_ribbon(mapping=aes(ymin=mu_lwr, ymax=mu_upr), alpha=0.4, fill="blue") +
  geom_line() +
  ggtitle("Effect of Median Age of Marriage at Mean Marriage Rate") +
  theme_bw(base_size=14)
```

## What do we learn about the effects of Median Age and Marriage Rate?
```{r cf_plot2}
ggplot(data=cf_data, 
       mapping=aes(x=MedianAgeMarriage.s, y=Divorce,
                   ymin = pred_lwr, ymax = pred_upr)) +
  geom_ribbon(alpha=0.2) +
  geom_ribbon(mapping=aes(ymin=mu_lwr, ymax=mu_upr), alpha=0.4, fill="blue") +
  geom_line() +
  ggtitle("Effect of Median Age of Different Marriage Rates") +
  theme_bw(base_size=14) +
  facet_wrap(~Marriage.s)
```

## Posterior Prediction: Assessing Fit
- Good ole' Observed v. Residual, but now with moar error!  
\
- Residuals by groups  
\
- Residuals by other candidate predictors

## Getting Residuals
```{r fit-res, echo=TRUE}
mu <- link(fit, refresh=0)

#Get residual info
WaffleDivorce <- WaffleDivorce %>%
  mutate(Divorce_mu =  apply(mu, 2, mean),
         
         Divorce_mu_lwr = apply(mu, 2, HPDI)[1,],
         Divorce_mu_upr = apply(mu, 2, HPDI)[2,],
         
         #residuals
         Divorce_res = Divorce - Divorce_mu,
         Divorce_res_lwr = Divorce - Divorce_mu_lwr,
         Divorce_res_upr = Divorce - Divorce_mu_upr)
```

## What do we learn here?
```{r plot_res}
ggplot(WaffleDivorce, 
       mapping=aes(x=Divorce, y=Divorce_mu,
                   ymin=Divorce_mu_lwr, ymax=Divorce_mu_upr)) +
  geom_pointrange() +
  theme_bw(base_size=17) +
  geom_abline(slope = 1, intercept = 0, lty=2) +
  annotate(x=11, y=7, label="Less Divorce than Expected", geom="text")+
  annotate(x=7.5, y=13, label="More Divorce than Expected", geom="text")
```

<div class="fragment">What is up with those outlying points?</div>

## Posterior Prediction: Where did things go wrong?
`postcheck` to see estimate, data, & fit and prediction error
```{r postValid, results="hide"}
par(mfrow=c(2,2))
sink(tempfile())
postcheck(fit)
sink()
par(mfrow=c(1,1), ask=FALSE)
```

## Other Predictors?
```{r waffleError}
ggplot(data=WaffleDivorce,
       mapping=aes(x=WaffleHouses, y=Divorce_res,
                   ymin=Divorce_res_lwr, ymax=Divorce_res_upr)) +
  geom_pointrange(shape=1) +
  theme_bw(base_size=17) +
  stat_smooth(method="lm")
```

## Today's Outline
1. Multiple Predictors in a Bayesian Framework  
    - How multiple predictors tease apart spurious and masked relationships  
\
2. Evaluating a Model with Multiple Predictors  
\
3. **Problems With Too Many Predictors**  
\
4. Categorical Variables


## Why not add everything?
<div id="right">
```{r}
glimmer(Divorce ~ MedianAgeMarriage.s+ Marriage.s + South + Population, data=WaffleDivorce)
```

</div>

<div id="left">
- Loss of precision
- Multicollinearity
- Loss of Interpretability
- Overfitting

</div>

## Loss of precision
```{r moar_coefs, cache=TRUE, results="hide"}
WaffleDivorce$Population.s <- scale(WaffleDivorce$Population1860)

sink(tempfile())
mods <- list(
  glimmer(Divorce ~ MedianAgeMarriage.s, data=WaffleDivorce),
  glimmer(Divorce ~ MedianAgeMarriage.s + Marriage.s, data=WaffleDivorce),
  glimmer(Divorce ~ MedianAgeMarriage.s+ Marriage.s + South, data=WaffleDivorce),
  glimmer(Divorce ~ MedianAgeMarriage.s+ Marriage.s + South + Population.s, data=WaffleDivorce))
sink()

set.seed(2016)
fits <- lapply(1:3, function(i) {print(i); 
  map(mods[[i]]$f, data=mods[[i]]$d)})

stdev <- sapply(fits, function(f) precis(f)@output[2,2])

plot(1:3, stdev, ylab="Std Dev of Median Marriage Age Coef", xlab="# of Parameters", type="b",cex.lab=1.3, cex=2)

```

## Bias-Variance Tradeoff
![](./images/14/parameter_bias_tradeoff.jpg)

## Multicollinearity and Crabs
![](./images/14/Florida-Stone-Crab.jpg)

```{r mvn}
set.seed(609)
crabs <- data.frame(rmvnorm2(100, Mu = c(5,5), sigma=c(1,1), Rho=matrix(c(1,.82, .82, 1), nrow=2)))
names(crabs) <- c("left_claw_size", "right_claw_size")
crabs$predation <- rnorm(100, rowSums(crabs)/2, 4)
pairs(crabs)
```
<span style="font-size=0.1em; top=100%">TreasureCoast.com</span>


## What did coefficeints for each claw size mean?
<div class="fragment">
<br><br>
<center>What do you gain by knowing one claw size after knowing the other claw size?</center>
</div>

## Do Crabs really not have an effect?
```{r crab_eff, cache=TRUE, results="hide"}
set.seed(12121)
crab_mod <- glimmer(predation ~ left_claw_size + right_claw_size, data=crabs)
crab_fit <- map(crab_mod$f, crab_mod$d, control=list(maxiter=5000))

plot(precis(crab_fit))
```

## The Problem of Correlated Coefficients

```{r crab_pairs}
pairs(crab_fit)
```

## The Information is there if you sum claw effects
```{r post_plots, fig.height=6, fig.width=9}
samp <- extract.samples(crab_fit) %>%
  mutate(sum_claw_effects = b_left_claw_size + b_right_claw_size) %>%
  select(b_left_claw_size, b_right_claw_size, sum_claw_effects) %>%
  gather(Coefficient, Value)

ggplot(samp, mapping=aes(x=Value, fill=Coefficient)) + 
    geom_density() +
  facet_wrap(~Coefficient) + theme_bw(base_size=14) 
```

## Today's Outline
1. Multiple Predictors in a Bayesian Framework  
    - How multiple predictors tease apart spurious and masked relationships  
\
2. Evaluating a Model with Multiple Predictors  
\
3. Problems With Too Many Predictors  
\
4. **Categorical Variables**


## Categorical Variables
- Lots of ways to write models with categorical variables  
\
- We all hate R's treatment contrasts  
\
- Two main ways to write a model

## Categorical Model Construction
1. Code each level as 1 or 0 if present/absent  
    - Need to have one baseline level  
    - Treatment contrasts!  
    - `predation <- a + b * is_crab`  

\
2. Index your categories  
    - Need to convert factors to levels with `as.numeric()`  
    - `predaion <- a[species]`  

## Monkies and Milk
![](./images/14/monkies_milk.jpg)

## Monkies and Milk Production
```{r milk, echo=TRUE}
data(milk)
head(milk)
```

## To easily make Variables
```{r make_monkey_clade, echo=TRUE}
mmat <- model.matrix.default(mass ~ clade, data=milk)
colnames(mmat) <- c("Ape", "New_World_Monkey", "Old_World_Monkey", "Strepsirrhine")
milk <- cbind(milk, mmat)

head(mmat)
```

## Original Milk Model
```{r ape_mod1, echo=TRUE}
milk_mod_1 <- alist(
  kcal.per.g ~ dnorm(mu, sigma),
  
  mu <- a*Ape + b1*New_World_Monkey +
    b2*Old_World_Monkey + b3*Strepsirrhine,
  
  a ~ dnorm(0.6, 10),
  b1 ~ dnorm(0,1),
  b2 ~ dnorm(0,1),
  b3 ~ dnorm(0,1),
  sigma ~ dunif(0,10)
)

milk_fit_1 <- map(milk_mod_1, data=milk)
```

## Milk Coefs: What does a and b mean?
```{r plot_ape_1}
plot(precis(milk_fit_1))
```

## To get the New World mean...
```{r combine, echo=TRUE}
samp_milk <- extract.samples(milk_fit_1)

new_world <- samp_milk$a + samp_milk$b1

#Deviation from Ape
HPDI(samp_milk$b1)

#New World Monkies
HPDI(new_world)
```

## A Better Way
```{r ape_2, echo=TRUE}
milk$clade_idx <- as.numeric(milk$clade)

#A new model
milk_mod_2 <- alist(
  kcal.per.g ~ dnorm(mu, sigma),
  
  #note the indexing!
  mu <- a[clade_idx],
  
  #four priors with one line!
  a[clade_idx] ~ dnorm(0.6, 10),
  sigma ~ dunif(0,10)
  
)

milk_fit_2 <- map(milk_mod_2, data=milk)
```

## Compare Results
```{r samp_compare, echo=TRUE}
#New World Monkies
#From Mod1
HPDI(new_world)

#From Mod2 - note indexing
samp_fit2 <- extract.samples(milk_fit_2)
HPDI(samp_fit2$a[,2])
```

## Today's Exercise
1. Build a model explaing the `kcal.per.g` of milk  
\
2. First try 2 continuous predictors  
\
3. Add clade  
\
4. Bonus: Can you make an interaction (try this *last*)
